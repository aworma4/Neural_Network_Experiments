{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26117f89-985e-420e-acfd-f02eecc4dd89",
   "metadata": {},
   "source": [
    "# Working off the Annotated Transformer:\n",
    "Reference notebook:\n",
    "http://nlp.seas.harvard.edu/annotated-transformer/#training\n",
    "\n",
    "Illustrated notebook:\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "OG article:\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Masking:\n",
    "https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c\n",
    "\n",
    "Tensor2Tensor - attention illustration:\n",
    "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e7a44e-d31a-41c2-9765-173853a6aa9e",
   "metadata": {},
   "source": [
    "I had a lot of trouble with virtual environments in windows jupyter lab - it appears using vend and conda commands don't interact with the jupyter lab launched from anaconda - at least not in a way I'm familiar with - so the solution is to create a virtual environment - pip install packages via a terminal - then load jupyter lab and launch it from anaconda. \n",
    "\n",
    "I'm also looking at launch jupyter lab without anaconda - as I think this would fix the issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d5459be-138f-4aac-9e5a-797544af558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#check version\n",
    "#print version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac2bd9fb-e557-4e34-a617-1b67e1f24dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version\n",
      "-------------------- ------------\n",
      "altair               4.1.0\n",
      "anyio                3.5.0\n",
      "argon2-cffi          21.3.0\n",
      "argon2-cffi-bindings 21.2.0\n",
      "asttokens            2.0.5\n",
      "attrs                22.1.0\n",
      "Babel                2.11.0\n",
      "backcall             0.2.0\n",
      "beautifulsoup4       4.11.1\n",
      "black                22.12.0\n",
      "bleach               4.1.0\n",
      "blis                 0.7.9\n",
      "brotlipy             0.7.0\n",
      "catalogue            2.0.8\n",
      "certifi              2022.12.7\n",
      "cffi                 1.15.1\n",
      "charset-normalizer   2.1.1\n",
      "click                8.1.3\n",
      "colorama             0.4.6\n",
      "comm                 0.1.2\n",
      "cryptography         38.0.1\n",
      "cymem                2.0.7\n",
      "debugpy              1.5.1\n",
      "decorator            5.1.1\n",
      "defusedxml           0.7.1\n",
      "docker-pycreds       0.4.0\n",
      "entrypoints          0.4\n",
      "executing            0.8.3\n",
      "fastjsonschema       2.16.2\n",
      "filelock             3.9.0\n",
      "flake8               6.0.0\n",
      "flit_core            3.6.0\n",
      "gitdb                4.0.10\n",
      "GitPython            3.1.30\n",
      "GPUtil               1.4.0\n",
      "huggingface-hub      0.11.1\n",
      "idna                 3.4\n",
      "importlib-metadata   4.11.3\n",
      "ipykernel            6.19.2\n",
      "ipython              8.7.0\n",
      "ipython-genutils     0.2.0\n",
      "ipywidgets           8.0.4\n",
      "jedi                 0.18.1\n",
      "Jinja2               3.1.2\n",
      "joblib               1.2.0\n",
      "json5                0.9.6\n",
      "jsonschema           4.17.3\n",
      "jupyter_client       7.4.8\n",
      "jupyter_core         5.1.2\n",
      "jupyter-server       1.23.4\n",
      "jupyterlab           3.5.2\n",
      "jupyterlab-pygments  0.1.2\n",
      "jupyterlab_server    2.16.5\n",
      "jupyterlab-widgets   3.0.5\n",
      "jupytext             1.13.0\n",
      "langcodes            3.3.0\n",
      "lxml                 4.9.1\n",
      "markdown-it-py       1.1.0\n",
      "MarkupSafe           2.1.1\n",
      "matplotlib-inline    0.1.6\n",
      "mccabe               0.7.0\n",
      "mdit-py-plugins      0.3.3\n",
      "mistune              0.8.4\n",
      "murmurhash           1.0.9\n",
      "mypy-extensions      0.4.3\n",
      "nbclassic            0.4.8\n",
      "nbclient             0.5.13\n",
      "nbconvert            6.5.4\n",
      "nbformat             5.7.1\n",
      "nest-asyncio         1.5.6\n",
      "notebook             6.5.2\n",
      "notebook_shim        0.2.2\n",
      "numpy                1.24.1\n",
      "packaging            22.0\n",
      "pandas               1.3.5\n",
      "pandocfilters        1.5.0\n",
      "parso                0.8.3\n",
      "pathspec             0.10.3\n",
      "pathtools            0.1.2\n",
      "pathy                0.10.1\n",
      "pickleshare          0.7.5\n",
      "pip                  22.3.1\n",
      "platformdirs         2.6.2\n",
      "preshed              3.0.8\n",
      "prometheus-client    0.14.1\n",
      "promise              2.3\n",
      "prompt-toolkit       3.0.36\n",
      "protobuf             4.21.12\n",
      "psutil               5.9.4\n",
      "pure-eval            0.2.2\n",
      "pycodestyle          2.10.0\n",
      "pycparser            2.21\n",
      "pydantic             1.8.2\n",
      "pyflakes             3.0.1\n",
      "Pygments             2.11.2\n",
      "pyOpenSSL            22.0.0\n",
      "pyrsistent           0.19.3\n",
      "PySocks              1.7.1\n",
      "python-dateutil      2.8.2\n",
      "pytz                 2022.7\n",
      "pywin32              305.1\n",
      "pywinpty             2.0.2\n",
      "PyYAML               6.0\n",
      "pyzmq                23.2.0\n",
      "regex                2022.10.31\n",
      "requests             2.28.1\n",
      "scikit-learn         1.2.0\n",
      "scipy                1.10.0\n",
      "Send2Trash           1.8.0\n",
      "sentencepiece        0.1.97\n",
      "sentry-sdk           1.12.1\n",
      "setproctitle         1.3.2\n",
      "setuptools           65.5.0\n",
      "shortuuid            1.0.11\n",
      "six                  1.16.0\n",
      "smart-open           6.3.0\n",
      "smmap                5.0.0\n",
      "sniffio              1.2.0\n",
      "soupsieve            2.3.2.post1\n",
      "spacy                3.2.0\n",
      "spacy-legacy         3.0.11\n",
      "spacy-loggers        1.0.4\n",
      "srsly                2.4.5\n",
      "stack-data           0.2.0\n",
      "terminado            0.17.1\n",
      "thinc                8.0.17\n",
      "threadpoolctl        3.1.0\n",
      "tinycss2             1.2.1\n",
      "tokenizers           0.13.2\n",
      "toml                 0.10.2\n",
      "tomli                2.0.1\n",
      "toolz                0.12.0\n",
      "torch                1.11.0+cu113\n",
      "torchdata            0.3.0\n",
      "torchtext            0.12.0\n",
      "tornado              6.2\n",
      "tqdm                 4.64.1\n",
      "traitlets            5.8.0\n",
      "transformers         4.25.1\n",
      "typer                0.4.2\n",
      "typing_extensions    4.4.0\n",
      "urllib3              1.26.13\n",
      "wandb                0.13.7\n",
      "wasabi               0.10.1\n",
      "wcwidth              0.2.5\n",
      "webencodings         0.5.1\n",
      "websocket-client     0.58.0\n",
      "wheel                0.37.1\n",
      "widgetsnbextension   4.0.5\n",
      "win-inet-pton        1.1.0\n",
      "wincertstore         0.2\n",
      "zipp                 3.11.0\n"
     ]
    }
   ],
   "source": [
    "#see current packages\n",
    "#!pip install -r requirments.txt --user  \n",
    "#need to reload jupyter lab for pip install to take effect\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "95de2aeb-a4c7-4317-aa71-57182a1a7b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AT030915\\anaconda3\\envs\\Annotated_Transformer\\python.exe\n",
      "C:\\Users\\AT030915\\anaconda3\\envs\\Annotated_Transformer\\lib/site-packages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "import inspect\n",
    "print(os.path.dirname(inspect.getfile(inspect))+\"/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f91b67d4-1337-4a50-bc56-d3bcb010174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os \n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt \n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP \n",
    "\n",
    "#set fasle to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "183d96d7-3f15-4fef-a20c-4b8a6c02f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convenience helper functions - \n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a265a1e-4649-4cd7-920f-00adbf6d0ff8",
   "metadata": {},
   "source": [
    "# Encoding/embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98ad38-fbeb-4047-be72-3456c2f1e000",
   "metadata": {},
   "source": [
    " \n",
    "Actually it looks like embedding/encoding might be interchangeable in some contextes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "28c643a9-e75d-4558-90a9-7b7cc7b87d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using super because we want access the the nn.Module class properties\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models. The src is the input to the encoder and the tgt is the input to the decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "09649d1a-908d-4b0e-927b-7a01c7d5b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        #nn.Linear = Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94eb3c7b-31cd-4dda-9772-befdd02a1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for copying sublayers\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4bce3a62-f666-48e5-a347-5fa9e9d9f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the encoder has a stack of N = 6 identical layers\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "#help(nn.ModuleList)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0b816fca-178e-42e5-835c-13b66e728742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use a layerNorm to create connection around each two- sublayers\n",
    "#reduces computation time by normlising the activation of the neurons \n",
    "\n",
    "#the output of each sub-layer is LayerNorm(x+Sublayer(x))LayerNorm(x+Sublayer(x)), where Sublayer(x)Sublayer(x) is the function implemented by the sub-layer itself\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        #create activation and biases initialised as 1 and 0 respectively\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "983e2b75-b1f5-439f-97be-3a93772d8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the residual connection \n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        #randomly zeroes some of the elements of the input tensor for a given probability \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "#help(nn.Dropout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7bd54042-40a2-43b7-8642-8225ac9aab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now build the encoder class\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf0a03-67e8-4fe4-8794-77b2ea8d1c03",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "contains stack of N=6 identical layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2a5cd658-bc5e-4112-ad81-394a5748a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6dc03911-74c6-4ac0-bed4-d0bdb2071ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        #see paper diagram but you have the 2nd set of nodes of the encoder feeding into the second set of notes of the decoder - then 1 more set of decoder nodes afterwards\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3dff2da4-632a-4181-8981-8637dd05b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask to stop the attention prediction depending on outputs at positions greater than i\n",
    "#idea being you have a self attention sub layer (the first layer) in the decoder but you need to make sure that\n",
    "#predictions can only depend on the previous i words\n",
    "#essentially if you are trying to translate the 4th word then you can't use what's already in that position to predict what should be translated there.\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d5d8ac78-349d-4486-b558-4ee060ba3b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-b11cd6568e9b4919b7e5d3d3af79ec30\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-b11cd6568e9b4919b7e5d3d3af79ec30\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-b11cd6568e9b4919b7e5d3d3af79ec30\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ac10fa76400c18dafea2d104267bb03d\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"Subsequent Mask\", \"scale\": {\"scheme\": \"viridis\"}}, \"x\": {\"type\": \"ordinal\", \"field\": \"Window\"}, \"y\": {\"type\": \"ordinal\", \"field\": \"Masking\"}}, \"height\": 250, \"selection\": {\"selector017\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 250, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-ac10fa76400c18dafea2d104267bb03d\": [{\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 0}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 1}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 2}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 3}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 4}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 0, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 1, \"Masking\": 0}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 1}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 2}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 3}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 4}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 1, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 2, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 2, \"Masking\": 1}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 2}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 3}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 4}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 2, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 3, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 3, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 3, \"Masking\": 2}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 3}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 4}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 3, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 4, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 4, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 4, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 4, \"Masking\": 3}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 4}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 4, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 5, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 5, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 5, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 5, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 5, \"Masking\": 4}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 5, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 6, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 6, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 6, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 6, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 6, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 6, \"Masking\": 5}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 6, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 7, \"Masking\": 6}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 7, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 8, \"Masking\": 7}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 8, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 9, \"Masking\": 8}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 9, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 10, \"Masking\": 9}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 10, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 11, \"Masking\": 10}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 11, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 12, \"Masking\": 11}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 12, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 13, \"Masking\": 12}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 13, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 12}, {\"Subsequent Mask\": false, \"Window\": 14, \"Masking\": 13}, {\"Subsequent Mask\": true, \"Window\": 14, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 14, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 14, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 14, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 14, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 14, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 12}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 13}, {\"Subsequent Mask\": false, \"Window\": 15, \"Masking\": 14}, {\"Subsequent Mask\": true, \"Window\": 15, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 15, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 15, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 15, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 15, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 12}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 13}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 14}, {\"Subsequent Mask\": false, \"Window\": 16, \"Masking\": 15}, {\"Subsequent Mask\": true, \"Window\": 16, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 16, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 16, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 16, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 12}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 13}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 14}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 15}, {\"Subsequent Mask\": false, \"Window\": 17, \"Masking\": 16}, {\"Subsequent Mask\": true, \"Window\": 17, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 17, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 17, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 12}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 13}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 14}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 15}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 16}, {\"Subsequent Mask\": false, \"Window\": 18, \"Masking\": 17}, {\"Subsequent Mask\": true, \"Window\": 18, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 18, \"Masking\": 19}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 0}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 1}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 2}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 3}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 4}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 5}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 6}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 7}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 8}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 9}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 10}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 11}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 12}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 13}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 14}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 15}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 16}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 17}, {\"Subsequent Mask\": false, \"Window\": 19, \"Masking\": 18}, {\"Subsequent Mask\": true, \"Window\": 19, \"Masking\": 19}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the attention mask\n",
    "#i.e. the positions each tgt (target) word is allowed to look at\n",
    "\n",
    "def example_mask():\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(20)\n",
    "            for x in range(20)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "            alt.X(\"Window:O\"),\n",
    "            alt.Y(\"Masking:O\"),\n",
    "            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "#Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.\n",
    "show_example(example_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d5c7a-a30e-4bdc-9ac0-fac4d5365f81",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "1.  \"encoder-decoder\" attention layers - query is from the previous decoder layer and memory keys and avlues from the output of the encoder - means every position in the decoder can attend over all positions in the input sequence.\n",
    "2.  self attention encoder - key, value, query from the previous layer.\n",
    "3.  self attention decoder - ditto above except prevent leftward information flow in decoder - this is done usign the mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c401ea7d-1a66-43be-bfa9-d9b3aee02969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create attention function\n",
    "\n",
    "def attention(query, key, value, mask =None, dropout=None):\n",
    "    \"Comput 'scaled dot product attention\"\n",
    "    #refers to size of last dimension .size(-1) does\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        #replaces with -1e9 - numerical infinity\n",
    "        scores = scores.masked_fill(mask == 0 , -1e9)\n",
    "    \n",
    "    \n",
    "    #soft max over the last dimension of the matrix\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "78184494-17b7-4a22-b179-2d1d23ca9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiheaded atentions allows model to view information from different representation subspaces at different positiosn - these are then averaged \n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        #assert dimensions of model are a multiple of h, the number of heads \n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a1082-75af-44e5-bc62-74eb22e2cd52",
   "metadata": {},
   "source": [
    "# Position-wise Feed-Foward Networks\n",
    "\n",
    "Feeds worward with two linear transformation and a ReLU activation in between - this is so the neural network can't be linearly collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b53bd2df-03e6-46b6-8211-b00fbbb04261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70798cd-7530-4b6f-b577-8d920739dd07",
   "metadata": {},
   "source": [
    "# Embeddings and Softmax  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a0f0e8f-9785-4663-9196-99d4c4018dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5cfc24-e089-4560-9376-91b59c9b8868",
   "metadata": {},
   "source": [
    "# Positional Encoding \n",
    "So we retain the location of a word as well as the attention that word applied to all of the words within a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "07f5c866-e639-413b-bf94-e38eccdc74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        \n",
    "        #arrange is similar to a linspace function \n",
    "        #the divisor of the arguement of the trig function has a log applied ot it\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9879f2ac-d4db-42b3-887f-bb400c128faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ff40968c0b6640829f7eee680a933142\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ff40968c0b6640829f7eee680a933142\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ff40968c0b6640829f7eee680a933142\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-87ea1d39691af03b13d4cefede79865c\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"dimension\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"position\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"embedding\"}}, \"selection\": {\"selector018\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-87ea1d39691af03b13d4cefede79865c\": [{\"embedding\": 0.0, \"dimension\": 0, \"position\": 0}, {\"embedding\": 0.8414709568023682, \"dimension\": 0, \"position\": 1}, {\"embedding\": 0.9092974066734314, \"dimension\": 0, \"position\": 2}, {\"embedding\": 0.14112000167369843, \"dimension\": 0, \"position\": 3}, {\"embedding\": -0.756802499294281, \"dimension\": 0, \"position\": 4}, {\"embedding\": -0.9589242935180664, \"dimension\": 0, \"position\": 5}, {\"embedding\": -0.279415488243103, \"dimension\": 0, \"position\": 6}, {\"embedding\": 0.6569865942001343, \"dimension\": 0, \"position\": 7}, {\"embedding\": 0.9893582463264465, \"dimension\": 0, \"position\": 8}, {\"embedding\": 0.41211849451065063, \"dimension\": 0, \"position\": 9}, {\"embedding\": -0.5440211296081543, \"dimension\": 0, \"position\": 10}, {\"embedding\": -0.9999902248382568, \"dimension\": 0, \"position\": 11}, {\"embedding\": -0.5365729331970215, \"dimension\": 0, \"position\": 12}, {\"embedding\": 0.4201670289039612, \"dimension\": 0, \"position\": 13}, {\"embedding\": 0.9906073808670044, \"dimension\": 0, \"position\": 14}, {\"embedding\": 0.6502878665924072, \"dimension\": 0, \"position\": 15}, {\"embedding\": -0.2879033088684082, \"dimension\": 0, \"position\": 16}, {\"embedding\": -0.9613974690437317, \"dimension\": 0, \"position\": 17}, {\"embedding\": -0.7509872317314148, \"dimension\": 0, \"position\": 18}, {\"embedding\": 0.14987720549106598, \"dimension\": 0, \"position\": 19}, {\"embedding\": 0.9129452705383301, \"dimension\": 0, \"position\": 20}, {\"embedding\": 0.8366556167602539, \"dimension\": 0, \"position\": 21}, {\"embedding\": -0.008851309306919575, \"dimension\": 0, \"position\": 22}, {\"embedding\": -0.8462203741073608, \"dimension\": 0, \"position\": 23}, {\"embedding\": -0.9055783748626709, \"dimension\": 0, \"position\": 24}, {\"embedding\": -0.13235175609588623, \"dimension\": 0, \"position\": 25}, {\"embedding\": 0.7625584602355957, \"dimension\": 0, \"position\": 26}, {\"embedding\": 0.9563759565353394, \"dimension\": 0, \"position\": 27}, {\"embedding\": 0.2709057927131653, \"dimension\": 0, \"position\": 28}, {\"embedding\": -0.6636338829994202, \"dimension\": 0, \"position\": 29}, {\"embedding\": -0.9880316257476807, \"dimension\": 0, \"position\": 30}, {\"embedding\": -0.4040376543998718, \"dimension\": 0, \"position\": 31}, {\"embedding\": 0.5514267086982727, \"dimension\": 0, \"position\": 32}, {\"embedding\": 0.9999118447303772, \"dimension\": 0, \"position\": 33}, {\"embedding\": 0.529082715511322, \"dimension\": 0, \"position\": 34}, {\"embedding\": -0.4281826615333557, \"dimension\": 0, \"position\": 35}, {\"embedding\": -0.9917788505554199, \"dimension\": 0, \"position\": 36}, {\"embedding\": -0.6435381174087524, \"dimension\": 0, \"position\": 37}, {\"embedding\": 0.2963685691356659, \"dimension\": 0, \"position\": 38}, {\"embedding\": 0.9637953639030457, \"dimension\": 0, \"position\": 39}, {\"embedding\": 0.7451131343841553, \"dimension\": 0, \"position\": 40}, {\"embedding\": -0.15862266719341278, \"dimension\": 0, \"position\": 41}, {\"embedding\": -0.9165215492248535, \"dimension\": 0, \"position\": 42}, {\"embedding\": -0.8317747116088867, \"dimension\": 0, \"position\": 43}, {\"embedding\": 0.017701925709843636, \"dimension\": 0, \"position\": 44}, {\"embedding\": 0.8509035110473633, \"dimension\": 0, \"position\": 45}, {\"embedding\": 0.9017883539199829, \"dimension\": 0, \"position\": 46}, {\"embedding\": 0.12357312440872192, \"dimension\": 0, \"position\": 47}, {\"embedding\": -0.7682546377182007, \"dimension\": 0, \"position\": 48}, {\"embedding\": -0.9537526369094849, \"dimension\": 0, \"position\": 49}, {\"embedding\": -0.2623748481273651, \"dimension\": 0, \"position\": 50}, {\"embedding\": 0.6702291965484619, \"dimension\": 0, \"position\": 51}, {\"embedding\": 0.9866275787353516, \"dimension\": 0, \"position\": 52}, {\"embedding\": 0.3959251642227173, \"dimension\": 0, \"position\": 53}, {\"embedding\": -0.558789074420929, \"dimension\": 0, \"position\": 54}, {\"embedding\": -0.9997551441192627, \"dimension\": 0, \"position\": 55}, {\"embedding\": -0.5215510129928589, \"dimension\": 0, \"position\": 56}, {\"embedding\": 0.4361647665500641, \"dimension\": 0, \"position\": 57}, {\"embedding\": 0.9928726553916931, \"dimension\": 0, \"position\": 58}, {\"embedding\": 0.6367380023002625, \"dimension\": 0, \"position\": 59}, {\"embedding\": -0.30481061339378357, \"dimension\": 0, \"position\": 60}, {\"embedding\": -0.966117799282074, \"dimension\": 0, \"position\": 61}, {\"embedding\": -0.7391806840896606, \"dimension\": 0, \"position\": 62}, {\"embedding\": 0.1673557013273239, \"dimension\": 0, \"position\": 63}, {\"embedding\": 0.9200260639190674, \"dimension\": 0, \"position\": 64}, {\"embedding\": 0.82682865858078, \"dimension\": 0, \"position\": 65}, {\"embedding\": -0.026551153510808945, \"dimension\": 0, \"position\": 66}, {\"embedding\": -0.8555199503898621, \"dimension\": 0, \"position\": 67}, {\"embedding\": -0.8979277014732361, \"dimension\": 0, \"position\": 68}, {\"embedding\": -0.11478481441736221, \"dimension\": 0, \"position\": 69}, {\"embedding\": 0.7738906741142273, \"dimension\": 0, \"position\": 70}, {\"embedding\": 0.9510546326637268, \"dimension\": 0, \"position\": 71}, {\"embedding\": 0.2538233697414398, \"dimension\": 0, \"position\": 72}, {\"embedding\": -0.6767719388008118, \"dimension\": 0, \"position\": 73}, {\"embedding\": -0.9851462841033936, \"dimension\": 0, \"position\": 74}, {\"embedding\": -0.38778164982795715, \"dimension\": 0, \"position\": 75}, {\"embedding\": 0.5661076307296753, \"dimension\": 0, \"position\": 76}, {\"embedding\": 0.9995201826095581, \"dimension\": 0, \"position\": 77}, {\"embedding\": 0.5139784812927246, \"dimension\": 0, \"position\": 78}, {\"embedding\": -0.4441126585006714, \"dimension\": 0, \"position\": 79}, {\"embedding\": -0.9938886761665344, \"dimension\": 0, \"position\": 80}, {\"embedding\": -0.6298879981040955, \"dimension\": 0, \"position\": 81}, {\"embedding\": 0.3132287859916687, \"dimension\": 0, \"position\": 82}, {\"embedding\": 0.9683644771575928, \"dimension\": 0, \"position\": 83}, {\"embedding\": 0.7331902980804443, \"dimension\": 0, \"position\": 84}, {\"embedding\": -0.17607562243938446, \"dimension\": 0, \"position\": 85}, {\"embedding\": -0.923458456993103, \"dimension\": 0, \"position\": 86}, {\"embedding\": -0.8218178153038025, \"dimension\": 0, \"position\": 87}, {\"embedding\": 0.03539830446243286, \"dimension\": 0, \"position\": 88}, {\"embedding\": 0.8600693941116333, \"dimension\": 0, \"position\": 89}, {\"embedding\": 0.8939966559410095, \"dimension\": 0, \"position\": 90}, {\"embedding\": 0.10598751157522202, \"dimension\": 0, \"position\": 91}, {\"embedding\": -0.7794660925865173, \"dimension\": 0, \"position\": 92}, {\"embedding\": -0.9482821226119995, \"dimension\": 0, \"position\": 93}, {\"embedding\": -0.24525198340415955, \"dimension\": 0, \"position\": 94}, {\"embedding\": 0.6832616925239563, \"dimension\": 0, \"position\": 95}, {\"embedding\": 0.9835877418518066, \"dimension\": 0, \"position\": 96}, {\"embedding\": 0.3796077370643616, \"dimension\": 0, \"position\": 97}, {\"embedding\": -0.5733819007873535, \"dimension\": 0, \"position\": 98}, {\"embedding\": -0.9992068409919739, \"dimension\": 0, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 3, \"position\": 0}, {\"embedding\": 0.921796441078186, \"dimension\": 3, \"position\": 1}, {\"embedding\": 0.6994173526763916, \"dimension\": 3, \"position\": 2}, {\"embedding\": 0.36764445900917053, \"dimension\": 3, \"position\": 3}, {\"embedding\": -0.021630670875310898, \"dimension\": 3, \"position\": 4}, {\"embedding\": -0.40752261877059937, \"dimension\": 3, \"position\": 5}, {\"embedding\": -0.7296751141548157, \"dimension\": 3, \"position\": 6}, {\"embedding\": -0.9377012848854065, \"dimension\": 3, \"position\": 7}, {\"embedding\": -0.9990642070770264, \"dimension\": 3, \"position\": 8}, {\"embedding\": -0.9041665196418762, \"dimension\": 3, \"position\": 9}, {\"embedding\": -0.6678506731987, \"dimension\": 3, \"position\": 10}, {\"embedding\": -0.3270781338214874, \"dimension\": 3, \"position\": 11}, {\"embedding\": 0.06485152989625931, \"dimension\": 3, \"position\": 12}, {\"embedding\": 0.44663795828819275, \"dimension\": 3, \"position\": 13}, {\"embedding\": 0.7585673928260803, \"dimension\": 3, \"position\": 14}, {\"embedding\": 0.9518511295318604, \"dimension\": 3, \"position\": 15}, {\"embedding\": 0.9962586760520935, \"dimension\": 3, \"position\": 16}, {\"embedding\": 0.8848443627357483, \"dimension\": 3, \"position\": 17}, {\"embedding\": 0.6350342035293579, \"dimension\": 3, \"position\": 18}, {\"embedding\": 0.2858997881412506, \"dimension\": 3, \"position\": 19}, {\"embedding\": -0.10795101523399353, \"dimension\": 3, \"position\": 20}, {\"embedding\": -0.4849175214767456, \"dimension\": 3, \"position\": 21}, {\"embedding\": -0.7860397696495056, \"dimension\": 3, \"position\": 22}, {\"embedding\": -0.9642193913459778, \"dimension\": 3, \"position\": 23}, {\"embedding\": -0.9915885329246521, \"dimension\": 3, \"position\": 24}, {\"embedding\": -0.8638659119606018, \"dimension\": 3, \"position\": 25}, {\"embedding\": -0.601029098033905, \"dimension\": 3, \"position\": 26}, {\"embedding\": -0.24418634176254272, \"dimension\": 3, \"position\": 27}, {\"embedding\": 0.15084894001483917, \"dimension\": 3, \"position\": 28}, {\"embedding\": 0.522289514541626, \"dimension\": 3, \"position\": 29}, {\"embedding\": 0.8120411038398743, \"dimension\": 3, \"position\": 30}, {\"embedding\": 0.9747832417488098, \"dimension\": 3, \"position\": 31}, {\"embedding\": 0.9850626587867737, \"dimension\": 3, \"position\": 32}, {\"embedding\": 0.8412709832191467, \"dimension\": 3, \"position\": 33}, {\"embedding\": 0.565899133682251, \"dimension\": 3, \"position\": 34}, {\"embedding\": 0.20201590657234192, \"dimension\": 3, \"position\": 35}, {\"embedding\": -0.19346313178539276, \"dimension\": 3, \"position\": 36}, {\"embedding\": -0.5586840510368347, \"dimension\": 3, \"position\": 37}, {\"embedding\": -0.83652263879776, \"dimension\": 3, \"position\": 38}, {\"embedding\": -0.9835227727890015, \"dimension\": 3, \"position\": 39}, {\"embedding\": -0.9766931533813477, \"dimension\": 3, \"position\": 40}, {\"embedding\": -0.8171020746231079, \"dimension\": 3, \"position\": 41}, {\"embedding\": -0.529710054397583, \"dimension\": 3, \"position\": 42}, {\"embedding\": -0.1594673991203308, \"dimension\": 3, \"position\": 43}, {\"embedding\": 0.23571711778640747, \"dimension\": 3, \"position\": 44}, {\"embedding\": 0.5940337181091309, \"dimension\": 3, \"position\": 45}, {\"embedding\": 0.8594381213188171, \"dimension\": 3, \"position\": 46}, {\"embedding\": 0.9904215335845947, \"dimension\": 3, \"position\": 47}, {\"embedding\": 0.9664957523345947, \"dimension\": 3, \"position\": 48}, {\"embedding\": 0.7914028763771057, \"dimension\": 3, \"position\": 49}, {\"embedding\": 0.4925287365913391, \"dimension\": 3, \"position\": 50}, {\"embedding\": 0.11662137508392334, \"dimension\": 3, \"position\": 51}, {\"embedding\": -0.2775281071662903, \"dimension\": 3, \"position\": 52}, {\"embedding\": -0.6282700896263123, \"dimension\": 3, \"position\": 53}, {\"embedding\": -0.880746066570282, \"dimension\": 3, \"position\": 54}, {\"embedding\": -0.9954668283462524, \"dimension\": 3, \"position\": 55}, {\"embedding\": -0.9544892311096191, \"dimension\": 3, \"position\": 56}, {\"embedding\": -0.7642236351966858, \"dimension\": 3, \"position\": 57}, {\"embedding\": -0.4544273316860199, \"dimension\": 3, \"position\": 58}, {\"embedding\": -0.07355520129203796, \"dimension\": 3, \"position\": 59}, {\"embedding\": 0.3188214898109436, \"dimension\": 3, \"position\": 60}, {\"embedding\": 0.6613321304321289, \"dimension\": 3, \"position\": 61}, {\"embedding\": 0.9004047513008118, \"dimension\": 3, \"position\": 62}, {\"embedding\": 0.9986488223075867, \"dimension\": 3, \"position\": 63}, {\"embedding\": 0.9406968355178833, \"dimension\": 3, \"position\": 64}, {\"embedding\": 0.7356129288673401, \"dimension\": 3, \"position\": 65}, {\"embedding\": 0.41547372937202454, \"dimension\": 3, \"position\": 66}, {\"embedding\": 0.03035326674580574, \"dimension\": 3, \"position\": 67}, {\"embedding\": -0.3595164120197296, \"dimension\": 3, \"position\": 68}, {\"embedding\": -0.69315505027771, \"dimension\": 3, \"position\": 69}, {\"embedding\": -0.9183791279792786, \"dimension\": 3, \"position\": 70}, {\"embedding\": -0.9999619722366333, \"dimension\": 3, \"position\": 71}, {\"embedding\": -0.9251440167427063, \"dimension\": 3, \"position\": 72}, {\"embedding\": -0.7056267857551575, \"dimension\": 3, \"position\": 73}, {\"embedding\": -0.3757442831993103, \"dimension\": 3, \"position\": 74}, {\"embedding\": 0.012907381169497967, \"dimension\": 3, \"position\": 75}, {\"embedding\": 0.3995402455329895, \"dimension\": 3, \"position\": 76}, {\"embedding\": 0.7236820459365845, \"dimension\": 3, \"position\": 77}, {\"embedding\": 0.9346339702606201, \"dimension\": 3, \"position\": 78}, {\"embedding\": 0.9994035363197327, \"dimension\": 3, \"position\": 79}, {\"embedding\": 0.9078590273857117, \"dimension\": 3, \"position\": 80}, {\"embedding\": 0.6743186712265015, \"dimension\": 3, \"position\": 81}, {\"embedding\": 0.3353116810321808, \"dimension\": 3, \"position\": 82}, {\"embedding\": -0.0561438724398613, \"dimension\": 3, \"position\": 83}, {\"embedding\": -0.43881458044052124, \"dimension\": 3, \"position\": 84}, {\"embedding\": -0.752854585647583, \"dimension\": 3, \"position\": 85}, {\"embedding\": -0.9491403102874756, \"dimension\": 3, \"position\": 86}, {\"embedding\": -0.9969748258590698, \"dimension\": 3, \"position\": 87}, {\"embedding\": -0.888874888420105, \"dimension\": 3, \"position\": 88}, {\"embedding\": -0.641749918460846, \"dimension\": 3, \"position\": 89}, {\"embedding\": -0.2942478656768799, \"dimension\": 3, \"position\": 90}, {\"embedding\": 0.09927338361740112, \"dimension\": 3, \"position\": 91}, {\"embedding\": 0.47726768255233765, \"dimension\": 3, \"position\": 92}, {\"embedding\": 0.7806168794631958, \"dimension\": 3, \"position\": 93}, {\"embedding\": 0.9618696570396423, \"dimension\": 3, \"position\": 94}, {\"embedding\": 0.9926798343658447, \"dimension\": 3, \"position\": 95}, {\"embedding\": 0.868228018283844, \"dimension\": 3, \"position\": 96}, {\"embedding\": 0.6079801917076111, \"dimension\": 3, \"position\": 97}, {\"embedding\": 0.25263699889183044, \"dimension\": 3, \"position\": 98}, {\"embedding\": -0.14221711456775665, \"dimension\": 3, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305388361215591, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585683166980743, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971213936805725, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027159094810486, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695962131023407, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.4274499714374542, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.4836025834083557, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305912017822, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899181365966797, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396579146385193, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.6868520379066467, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.7313126921653748, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728629112243652, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372921943665, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827703475952, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784590363502502, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068393111228943, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316105246543884, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.9526742100715637, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699464440345764, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585619926453, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965131759644, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759093284607, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516780853271, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485803842544556, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267339110374451, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.9011994004249573, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720782399177551, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865393638611, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035537600517273, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644230127334595, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222501039505005, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772029399871826, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294605135917664, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.57921302318573, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266605615615845, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.4720119535923004, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548484563827515, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.3573042154312134, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770180583000183, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691439628601074, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.17518411576747894, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.1127568930387497, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988069087266922, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013194027356803417, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621623575687408, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389348804950714, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110084116458893, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.2624664604663849, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227875530719757, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.3818237781524658, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393406808376312, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.49510911107063293, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.5489069223403931, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.6005204319953918, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497439742088318, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963817477226257, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402478456497192, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811681628227234, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189795017242432, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535317182540894, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846868872642517, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.9123212099075317, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.936324954032898, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.956602156162262, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730724096298218, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699705123901, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990625381469727, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.9893633723258972, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782225489616394, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.963188648223877, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443213939666748, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216960668563843, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954026699066162, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.8655455708503723, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.8322440981864929, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.795630156993866, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558501362800598, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130619883537292, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674357056617737, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191535592079163, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684073567390442, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153986215591431, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033912897109985, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.40344759821891785, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.3449500501155853, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508007526397705, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407560050487518, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.1621788740158081, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963719546794891, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669850528240204, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 9, \"position\": 0}, {\"embedding\": 0.9996845126152039, \"dimension\": 9, \"position\": 1}, {\"embedding\": 0.9987383484840393, \"dimension\": 9, \"position\": 2}, {\"embedding\": 0.9971620440483093, \"dimension\": 9, \"position\": 3}, {\"embedding\": 0.9949565529823303, \"dimension\": 9, \"position\": 4}, {\"embedding\": 0.9921233654022217, \"dimension\": 9, \"position\": 5}, {\"embedding\": 0.9886642694473267, \"dimension\": 9, \"position\": 6}, {\"embedding\": 0.9845813512802124, \"dimension\": 9, \"position\": 7}, {\"embedding\": 0.979877233505249, \"dimension\": 9, \"position\": 8}, {\"embedding\": 0.9745548963546753, \"dimension\": 9, \"position\": 9}, {\"embedding\": 0.9686176776885986, \"dimension\": 9, \"position\": 10}, {\"embedding\": 0.9620693325996399, \"dimension\": 9, \"position\": 11}, {\"embedding\": 0.9549140334129333, \"dimension\": 9, \"position\": 12}, {\"embedding\": 0.9471561908721924, \"dimension\": 9, \"position\": 13}, {\"embedding\": 0.9388008117675781, \"dimension\": 9, \"position\": 14}, {\"embedding\": 0.929853081703186, \"dimension\": 9, \"position\": 15}, {\"embedding\": 0.9203187227249146, \"dimension\": 9, \"position\": 16}, {\"embedding\": 0.9102036952972412, \"dimension\": 9, \"position\": 17}, {\"embedding\": 0.8995144367218018, \"dimension\": 9, \"position\": 18}, {\"embedding\": 0.888257622718811, \"dimension\": 9, \"position\": 19}, {\"embedding\": 0.8764403462409973, \"dimension\": 9, \"position\": 20}, {\"embedding\": 0.8640701770782471, \"dimension\": 9, \"position\": 21}, {\"embedding\": 0.8511548042297363, \"dimension\": 9, \"position\": 22}, {\"embedding\": 0.8377023935317993, \"dimension\": 9, \"position\": 23}, {\"embedding\": 0.8237214684486389, \"dimension\": 9, \"position\": 24}, {\"embedding\": 0.8092208504676819, \"dimension\": 9, \"position\": 25}, {\"embedding\": 0.7942097187042236, \"dimension\": 9, \"position\": 26}, {\"embedding\": 0.7786974310874939, \"dimension\": 9, \"position\": 27}, {\"embedding\": 0.7626938819885254, \"dimension\": 9, \"position\": 28}, {\"embedding\": 0.7462091445922852, \"dimension\": 9, \"position\": 29}, {\"embedding\": 0.7292535305023193, \"dimension\": 9, \"position\": 30}, {\"embedding\": 0.7118379473686218, \"dimension\": 9, \"position\": 31}, {\"embedding\": 0.6939731240272522, \"dimension\": 9, \"position\": 32}, {\"embedding\": 0.6756705045700073, \"dimension\": 9, \"position\": 33}, {\"embedding\": 0.6569415926933289, \"dimension\": 9, \"position\": 34}, {\"embedding\": 0.6377981901168823, \"dimension\": 9, \"position\": 35}, {\"embedding\": 0.6182523965835571, \"dimension\": 9, \"position\": 36}, {\"embedding\": 0.598316490650177, \"dimension\": 9, \"position\": 37}, {\"embedding\": 0.5780031085014343, \"dimension\": 9, \"position\": 38}, {\"embedding\": 0.5573251247406006, \"dimension\": 9, \"position\": 39}, {\"embedding\": 0.5362954139709473, \"dimension\": 9, \"position\": 40}, {\"embedding\": 0.5149273872375488, \"dimension\": 9, \"position\": 41}, {\"embedding\": 0.4932345151901245, \"dimension\": 9, \"position\": 42}, {\"embedding\": 0.47123032808303833, \"dimension\": 9, \"position\": 43}, {\"embedding\": 0.44892895221710205, \"dimension\": 9, \"position\": 44}, {\"embedding\": 0.4263443350791931, \"dimension\": 9, \"position\": 45}, {\"embedding\": 0.4034906029701233, \"dimension\": 9, \"position\": 46}, {\"embedding\": 0.3803824186325073, \"dimension\": 9, \"position\": 47}, {\"embedding\": 0.35703423619270325, \"dimension\": 9, \"position\": 48}, {\"embedding\": 0.33346065878868103, \"dimension\": 9, \"position\": 49}, {\"embedding\": 0.309676855802536, \"dimension\": 9, \"position\": 50}, {\"embedding\": 0.2856976389884949, \"dimension\": 9, \"position\": 51}, {\"embedding\": 0.26153817772865295, \"dimension\": 9, \"position\": 52}, {\"embedding\": 0.23721356689929962, \"dimension\": 9, \"position\": 53}, {\"embedding\": 0.21273943781852722, \"dimension\": 9, \"position\": 54}, {\"embedding\": 0.18813106417655945, \"dimension\": 9, \"position\": 55}, {\"embedding\": 0.16340389847755432, \"dimension\": 9, \"position\": 56}, {\"embedding\": 0.13857373595237732, \"dimension\": 9, \"position\": 57}, {\"embedding\": 0.11365615576505661, \"dimension\": 9, \"position\": 58}, {\"embedding\": 0.08866674453020096, \"dimension\": 9, \"position\": 59}, {\"embedding\": 0.06362151354551315, \"dimension\": 9, \"position\": 60}, {\"embedding\": 0.03853613883256912, \"dimension\": 9, \"position\": 61}, {\"embedding\": 0.013426452875137329, \"dimension\": 9, \"position\": 62}, {\"embedding\": -0.011691824533045292, \"dimension\": 9, \"position\": 63}, {\"embedding\": -0.03680260479450226, \"dimension\": 9, \"position\": 64}, {\"embedding\": -0.061890166252851486, \"dimension\": 9, \"position\": 65}, {\"embedding\": -0.08693879842758179, \"dimension\": 9, \"position\": 66}, {\"embedding\": -0.11193246394395828, \"dimension\": 9, \"position\": 67}, {\"embedding\": -0.13685549795627594, \"dimension\": 9, \"position\": 68}, {\"embedding\": -0.16169220209121704, \"dimension\": 9, \"position\": 69}, {\"embedding\": -0.18642699718475342, \"dimension\": 9, \"position\": 70}, {\"embedding\": -0.2110440582036972, \"dimension\": 9, \"position\": 71}, {\"embedding\": -0.23552796244621277, \"dimension\": 9, \"position\": 72}, {\"embedding\": -0.25986337661743164, \"dimension\": 9, \"position\": 73}, {\"embedding\": -0.28403472900390625, \"dimension\": 9, \"position\": 74}, {\"embedding\": -0.30802688002586365, \"dimension\": 9, \"position\": 75}, {\"embedding\": -0.33182480931282043, \"dimension\": 9, \"position\": 76}, {\"embedding\": -0.3554132580757141, \"dimension\": 9, \"position\": 77}, {\"embedding\": -0.37877747416496277, \"dimension\": 9, \"position\": 78}, {\"embedding\": -0.4019027054309845, \"dimension\": 9, \"position\": 79}, {\"embedding\": -0.4247744679450989, \"dimension\": 9, \"position\": 80}, {\"embedding\": -0.44737815856933594, \"dimension\": 9, \"position\": 81}, {\"embedding\": -0.46969956159591675, \"dimension\": 9, \"position\": 82}, {\"embedding\": -0.4917246103286743, \"dimension\": 9, \"position\": 83}, {\"embedding\": -0.513439416885376, \"dimension\": 9, \"position\": 84}, {\"embedding\": -0.5348305106163025, \"dimension\": 9, \"position\": 85}, {\"embedding\": -0.5558839440345764, \"dimension\": 9, \"position\": 86}, {\"embedding\": -0.5765866637229919, \"dimension\": 9, \"position\": 87}, {\"embedding\": -0.5969256162643433, \"dimension\": 9, \"position\": 88}, {\"embedding\": -0.6168879270553589, \"dimension\": 9, \"position\": 89}, {\"embedding\": -0.6364610195159912, \"dimension\": 9, \"position\": 90}, {\"embedding\": -0.6556327939033508, \"dimension\": 9, \"position\": 91}, {\"embedding\": -0.6743906736373901, \"dimension\": 9, \"position\": 92}, {\"embedding\": -0.6927230954170227, \"dimension\": 9, \"position\": 93}, {\"embedding\": -0.7106184363365173, \"dimension\": 9, \"position\": 94}, {\"embedding\": -0.7280654311180115, \"dimension\": 9, \"position\": 95}, {\"embedding\": -0.7450531125068665, \"dimension\": 9, \"position\": 96}, {\"embedding\": -0.7615706920623779, \"dimension\": 9, \"position\": 97}, {\"embedding\": -0.7776079773902893, \"dimension\": 9, \"position\": 98}, {\"embedding\": -0.7931544184684753, \"dimension\": 9, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_positional():\n",
    "    pe = PositionalEncoding(20, 0)\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(100)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [0,3,6,9]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_positional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f98c5c-d31c-4a17-83ab-0826dbbc61b7",
   "metadata": {},
   "source": [
    "#  Define model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de4dad11-9f12-4cc2-84f9-502a32849533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_ff - inner layer dimensionality \n",
    "\n",
    "# #nn.init.xavier_uniform_: \n",
    "# Fills the input `Tensor` with values according to the method\n",
    "# described in `Understanding the difficulty of training deep feedforward\n",
    "# neural networks` - Glorot, X. & Bengio, Y. (2010), using a uniform\n",
    "# distribution. The resulting tensor will have values sampled from\n",
    "# :math:`\\mathcal{U}(-a, a)`\n",
    "\n",
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022b9ac-4082-4002-9605-d2e98bede3ec",
   "metadata": {},
   "source": [
    "# Inference testing \n",
    "- generate predictions of the model based on an untrained input - hence the results will be random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1d4f7b55-f054-4dec-bd3e-ed1155cff54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[ 0, 10]])\n"
     ]
    }
   ],
   "source": [
    "def inference_test(output_words = 9):\n",
    "    test_model = make_model(src_vocab =11, tgt_vocab = 11, N =2)\n",
    "    \n",
    "    #show the model architecture\n",
    "    test_model.eval()\n",
    "    \n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "    \n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    #print(\"memory shape: {}\".format(memory.shape))\n",
    "    #variable to store prediction\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "    \n",
    "    \n",
    "    #loop through \n",
    "    for i in range(output_words -1):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        \n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim =1)\n",
    "        next_word = next_word.data[0]\n",
    "        #fill_ = Fills self tensor with the specified value.\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim =1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)    \n",
    "\n",
    "inference_test(output_words = 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bb12bf37-c8ea-4124-b30b-c0eafe10dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[ 0, 10,  7,  6,  0]])\n",
      "Example Untrained Model Prediction: tensor([[0, 5, 4, 2, 4]])\n",
      "Example Untrained Model Prediction: tensor([[0, 9, 1, 2, 5]])\n",
      "Example Untrained Model Prediction: tensor([[0, 5, 6, 7, 2]])\n",
      "Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "def run_tests(loops = 10, output_words = 9):\n",
    "    for _ in range(loops):\n",
    "        inference_test(output_words)\n",
    "        \n",
    "(run_tests(5,5))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65192a-0b4e-47b1-a336-47f5861434dd",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5eab5-65e3-4896-9fcc-4df431aafc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54415add-761d-4224-befb-f59adc37fbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddb105-1747-47b0-b290-c5bb29220360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25f1a2-bcc9-42b3-8313-0f3a5dbe6f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b18cb4-636c-4d9b-9b3b-ed9032f0b8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7abd-b3c3-4294-8743-676122ecd6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500405a-080f-46b4-9cfa-e2ea8fe00493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
